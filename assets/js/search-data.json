{
  
    
        "post0": {
            "title": "_opgave 2 Uge 6",
            "content": "09:40 – Opgave 2 (35 minutter) . Besvar følgende spørgsmål, f.eks. vha. ”Ordet rundt”. Noter svarene ned idet I skal bruge . dem til ”Tre til te” senere. . Name five areas where deep learning is now the best in the world. . Natural language processing : Svare på spørgsmål, Dokument klassificering. . | Computer vision : Sat and drone image interpretation . | Medicine: MRI , X-Ray At finde tumorer* . | Games Chess and Go. . | Recommendation systems : Amazon . | | Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? . A set of processing units . | A state of activation . | An output function for each unit . | A pattern of connectivity among units . | A propagation rule for propagating patterns of activities through the network of connectivities . | An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit . | A learning rule whereby patterns of connectivity are modified by experience . | An environment within which the system must operate . | | What were the two theoretical misunderstandings that held back the field of neural networks? . In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful. . | Folk blev ved med at add’e flere lag fordi det teoretisk set kunne løse mere komplekse problemer but not how it works exactly. . | | What is a GPU? . Graphics Processing Unit. Made for a 3D environment. Handle 1000 tasks at the same time; Faster Training of Neural Network. . | | Why is it hard to use a traditional computer program to recognize images in a photo? . We don’t quite understand how we(peeps) recognize images and therefore it is difficult to make a traditional Computer program recognize. . | | What did Samuel mean by “weight assignment”? . WA is a particular choice of values. . | “Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ”” . | | What term do we normally use in deep learning for what Samuel called “weights”? . We use the term Model Parameters. . | | Draw a picture that summarizes Samuel’s view of a machine learning model. . | . :) . Why is it hard to understand why a deep learning model makes a particular prediction? . We don’t know what happens in the black box. . | | What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . | Universal approximation theorem. . What do you need in order to train a model? . Dataset and labels . | | How could a feedback loop impact the rollout of a predictive policing model? . Negative feedback loop; Positive Feedback Loop. . | We make a predictive policing model based on arrest and therefore we don’t predict crime but rather arrests. . | | Do we always have to use 224×224-pixel images with the cat recognition model? . No we do not. 224x224 is commonly used for historical reasons. . | | What is the difference between classification and regression? . Classification is used to predict a class or category. Fx type animal. . | Regression is used to predict numeric quantity. fx age of animal. . | | What is a validation set? What is a test set? Why do we need them? . Validation set do we not use for training, but for validation to prevent overfitting. . | Test sets bruges til at færdiggøre ens model . | | What will fastai do if you don’t provide a validation set? . FastAi will automatically create a validation dataset. It will randomly take 20% of the data and assign it as the validation set. . | | Can we always use a random sample for a validation set? Why or why not? . No. A validation set must not be random in order to have consistency between the different results. If a validation set was random we would not be able to determine whether or not the results differ due to the training set or the validation set. . | | What is overfitting? Provide an example . “Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data.” . | | What is a metric? How does it differ from “loss”? . The measure of performance is called ‘loss’, depends not only on the predictions but also the labels: . | A metric is a function that measures the quality of the model’s predictions using the validation set. . | | How can pretrained models help? . Pretrained models have been trained on other problems that may be quite similar to the current task, pretrained models are useful because they have already learned how to handle a lot. . | | What is the “head” of a model? . “When using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the “head” of the model.” . | | What kinds of features do the early layers of a CNN find? How about the later layers? . The first layer focuses on things such as diagonal or horizontal lines as well as edges and various gradients. It then weights these accordingly. . | The second layer will focus on corners, repeating lines and other simple patterns. . | The third layer is able to identify higher-level semantic components i.e. wheels, flower petals etc. . | The fourth layer can identify concepts such as faces. . | | Are image models only useful for photos? . “An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.”(sounds) . | For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file . | | What is an “architecture”? . “The functional form of the model is called its architecture” fx Convolutional Neural Network) . | | What is segmentation? . “Creating a model that can recognize the content of every individual pixel in an image is called segmentation.” . | We divide the picture into individual pixels. . | | What is y_range used for? When do we need it? . afgrænse dataen . | for the regression model : . | | What are “hyperparameters”? . | . Model version . | model architecture . | learning rates . | Data augmentation strategies . | And so on.. . | What’s the best way to avoid failures when using AI in an organization? . . | | Complete the Jupyter Notebook online appendix. (use the file app_jupyter.ipynb in Gradient or get it from github: https://oreil.ly/9uPZe) . | Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? . They are many times faster at running multiple threads calculating the type of mathematical equation over and over again used in Neural networks. Because it is the same way graphics are calculated. . | | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. . | * | .",
            "url": "https://domovou.github.io/fastpages_blog/2021/03/23/_Opgave-2-Uge-6.html",
            "relUrl": "/2021/03/23/_Opgave-2-Uge-6.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning Uge 10 Spørgsmål",
            "content": "1. Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? . Konceptet er kaldt for Presizing. CPU’en er hurtigere til at opskalere. Data augmentation er ofte brugt på billeder og i fastai bliver det gjort på GPU’en, dog kan dette lede til dårligere kvalitet af billedet (degradation osv). . | . item_tfms=Resize(460), . batch_tfms=aug_transforms(size=224, min_scale=0.75) . 2. Gennemgå Learn Regex: A Beginner’s Guide . 3. What are the two ways in which data is most commonly provided, for most deep learning datasets? . Individual files representing items of data, such as text documents or images, possibly organized into folders or with filenames representing information about those items . | A table of data, such as in CSV format, where each row is an item which may include filenames providing a connection between the data in the table and data in other formats, such as text documents and images . | . 4. Look up the documentation for L and try using a few of the new methods that it adds. . https://fastcore.fast.ai/foundation#L . Format displaying . | Det første der bliver vist er antal af elementer med en prefix # . | Kun de første enkelte elementer i listen bliver displayet, selvom der kunne være 100 . | . 5. Look up the documentation for the Python pathlib module and try using a few methods of the Path class. . . https://docs.python.org/3/library/pathlib.html . 6. Give two examples of ways that image transformations can degrade the quality of the data. . . læg her mærke til græsset i kanterne, ligesom at ansigtet i billedet til højre har mistet lidt af sin skarphed. . 7. What method does fastai provide to view the data in a DataLoaders? . DataLoader.show_batch . 8. What method does fastai provide to help you debug a DataBlock? . #hide_output . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), . get_items=get_image_files, . splitter=RandomSplitter(seed=42), . get_y=using_attr(RegexLabeller(r’(.+)_ d+.jpg$’), ‘name’)) . pets1.summary(path/”images”) . 9. Should you hold off on training a model until you have thoroughly cleaned your data? . No. It is best to create a baseline model as soon as possible. . 10. What are the two pieces that are combined into cross-entropy loss in PyTorch? . Cross Entropy Loss is a combination of a Softmax function and Negative Log Likelihood Loss. . 11. What are the two properties of activations that softmax ensures? Why is this important? . Den sikrer at summen altid er 1. Det betyder at den kun kan klassificere en label. . med andre ord normalisere Softmax vores inputs. . Den forstærker/forstørrer forskellen mellem to klassifiseringer. . 12. When might you want your activations to not have these two properties? . Hvis man vil have multi-label klassificeringer, skal summen ikke altid blive 1. . 13. Calculate the exp and softmax columns of &lt;&lt;bear_softmax&gt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook). . 14. Why can’t we use torch.where to create a loss function for datasets where our label can have more than two categories? . 15. What is the value of log(-2)? Why? . 16. What are two good rules of thumb for picking a learning rate from the learning rate finder? . 17. What two steps does the fine_tune method do? . 18. In Jupyter Notebook, how do you get the source code for a method or function? . Skriv ‘??’ efter navnet på metoden/funktionen. . 19. What are discriminative learning rates? . da ens første layer ikke nødvendigvis har brug for at være ens mest præcise layer, kan man med fordel gøre brug af discriminative læringskurver til at hjælpe en med at forbedre ens “training loss”. vær dog opmærksom på at ens “validation loss” kan stagnere eller endda blive farligere og man derfor kan få en model som er overmodig. . 20. How is a Python slice object interpreted when passed as a learning rate to fastai? . 21. Why is early stopping a poor choice when using 1cycle training? . Fordi epochs i midten opstår før learning raten får en chance for at nå de mindre værdier, hvor den virkelig kan finde det bedste resultat. . Because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Overfit . 22. What is the difference between resnet50 and resnet101? . Amount of epochs. The difference is thus 51 (101-50) . 23. What does to_fp16 do? . Nvidia specifik, tensor cores som dramatisk speeder op træningen (2 til 3 gange). . Et mindre præcist tal som kaldes half-precision floating point. . . .",
            "url": "https://domovou.github.io/fastpages_blog/2021/03/23/Machine-Learning-Uge-10-sp%C3%B8rgsm%C3%A5l.html",
            "relUrl": "/2021/03/23/Machine-Learning-Uge-10-sp%C3%B8rgsm%C3%A5l.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning Spørgsmål Uge 8",
            "content": "1. Explain how the “pixel similarity” approach to classifying digits works. . Finder gennemsnitlig værdi for hver individuelle pixel i billedet og laver et ideelt billede. Derefte sammenligner den med de ideelle billeder for at karakterisere tallet. . | . 2. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. . Måde at lave en liste (LINQ) . | . numbers = [1,2,3,4,5,6,7,8,9] odd = [x*2 for x in numbers if x%2==0] | . 3. What is a “rank-3 tensor”? . Tre dimensionel tensor . | . . 4. What are RMSE and L1 norm? . L1 norm: . | Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm . | (L2 norm) RMSE: . | Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. . | . 5. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. . data = [[1,2,3],[4,5,6],[7,8,9]] . tens = tensor(data) * 2 . tens[-2:, -2:] . | . eller: . . 6. What is broadcasting? . Broadcasting means it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. . | . 7. Are metrics generally calculated using the training set, or the validation set? Why? . Metrics are always calculated on the validation set. . | Fordi Metrics fortæller os om kvaliteten af vores model . | . 8. What is SGD? . Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). . | . 9. What are the seven steps in SGD for machine learning? . Initialize the weights. . | For each image, use these weights to predict whether it appears to be a 3 or a 7. . | Based on these predictions, calculate how good the model is (its loss). . | Calculate the gradient, which measures for each weight, how changing that weight would change the loss . | Step (that is, change) all the weights based on that calculation. . | Go back to the step 2, and repeat the process. . | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer). . | . 10. How do we initialize the weights in a model? . Random values . | . 11. What is “loss”? . A measure of how good the model is, chosen to drive training via SGD(Stochastic Gradient Descent) . | . 12. Why can’t we always use a high learning rate? . Risikere at “hoppe” (step over) vores optimal/local minimum . | Kan risikere at få en forværret loss . | Risiko for at den “bouncer” som kan resultere i at der skal betydeligt flere trainings til at få trænet med succes . | . 13. What is a “gradient”? . En gradient måler ændringen i alle vægtene i forhold til ændringen i error. . | Gradient is simply a vector which gives the direction of maximum rate of change. By taking steps in that direction, we hope to reach our optimal solution. . | . 14. Why can’t we use accuracy as a loss function? . Gradient can be written in this in mathematically as: (y_new - y_old) / (x_new - x_old). This gives us a good approximation of the gradient when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. The problem is that a small change in weights from x_old to x_new isn’t likely to cause any prediction to change, so (y_new - y_old) will almost always be 0. In other words, the gradient is 0 almost everywhere. . 15. What is the difference between a loss function and a metric? . Loss: A measure of how good the model is, chosen to drive training via SGD(Stochastic Gradient Descent) . Metric: A measurement of how good the model is, using the validation set, chosen for human consumption . A loss function is used to train your model. A metric is used to evaluate your model. . | A loss function is used during the learning process. A metric is used after the learning process . | . 16. What is the function to calculate new weights using a learning rate? . 17. What does the backward method do? .",
            "url": "https://domovou.github.io/fastpages_blog/2021/03/23/Deep-Learning-Sp%C3%B8rgsm%C3%A5l-Uge-8.html",
            "relUrl": "/2021/03/23/Deep-Learning-Sp%C3%B8rgsm%C3%A5l-Uge-8.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Uge 10",
            "content": ". !pip install -Uqq fastbook import fastbook fastbook.setup_book() . |████████████████████████████████| 727kB 7.5MB/s |████████████████████████████████| 51kB 6.1MB/s |████████████████████████████████| 1.2MB 13.0MB/s |████████████████████████████████| 194kB 34.2MB/s |████████████████████████████████| 51kB 8.3MB/s |████████████████████████████████| 61kB 9.4MB/s |████████████████████████████████| 776.8MB 23kB/s |████████████████████████████████| 12.8MB 244kB/s ERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you&#39;ll have torch 1.7.1 which is incompatible. Mounted at /content/gdrive . from fastbook import * . from fastai.vision.all import * path = untar_data(URLs.PETS) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/Birman_183.jpg&#39;),Path(&#39;images/Sphynx_125.jpg&#39;),Path(&#39;images/Bombay_117.jpg&#39;),Path(&#39;images/newfoundland_65.jpg&#39;),Path(&#39;images/British_Shorthair_31.jpg&#39;),Path(&#39;images/Ragdoll_252.jpg&#39;),Path(&#39;images/basset_hound_70.jpg&#39;),Path(&#39;images/Persian_30.jpg&#39;),Path(&#39;images/pomeranian_137.jpg&#39;),Path(&#39;images/Bengal_130.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;Birman&#39;] . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . #id interpolations #caption A comparison of fastai&#39;s data augmentation strategy (left) and the traditional approach (right). dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . dls.show_batch(nrows=1, ncols=10) . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying PILBase.create gives PILImage mode=RGB size=333x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying partial gives basset_hound applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(14) Final sample: (PILImage mode=RGB size=333x500, TensorCategory(14)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=333x500, TensorCategory(14)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(14)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(14)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.508895 | 1.112378 | 0.338972 | 41:45 | . epoch train_loss valid_loss error_rate time . 0 | 0.501431 | 0.857496 | 0.256428 | 57:19 | . 1 | 0.322268 | 0.678694 | 0.210419 | 56:46 | . x,y = dls.one_batch() . y . TensorCategory([ 7, 7, 0, 10, 5, 12, 19, 0, 24, 35, 6, 19, 33, 9, 27, 27, 34, 13, 11, 34, 11, 19, 18, 35, 31, 33, 34, 27, 4, 32, 34, 19, 12, 16, 17, 0, 11, 23, 29, 13, 25, 14, 36, 31, 5, 29, 12, 23, 22, 28, 4, 30, 25, 16, 17, 20, 24, 8, 4, 27, 34, 25, 19, 11], device=&#39;cuda:0&#39;) . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([3.3819e-08, 1.7169e-08, 5.6901e-07, 1.0827e-08, 7.6176e-06, 1.1637e-08, 1.3479e-04, 9.9984e-01, 9.4581e-06, 3.1539e-09, 2.4227e-09, 2.5317e-08, 1.1819e-08, 9.4395e-09, 4.8617e-09, 2.5222e-09, 1.3380e-08, 2.5421e-09, 1.5080e-08, 1.8861e-09, 6.7784e-10, 1.1486e-07, 1.6067e-06, 1.4473e-07, 3.7542e-08, 1.2817e-07, 5.8590e-10, 1.1069e-07, 6.6972e-08, 4.8398e-08, 8.6074e-09, 4.5674e-08, 8.4219e-09, 1.1441e-08, 1.6261e-09, 1.3757e-07, 5.1108e-08]) . len(preds[0]),preds[0].sum() . (37, tensor(1.)) . plot_function(torch.sigmoid, min=-4,max=4) . /usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . torch.random.manual_seed(42); . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . plot_function(torch.log, min=0,max=4) . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;beagle&#39;, &#39;basset_hound&#39;, 18), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 12), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 11), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 10), (&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 8), (&#39;Ragdoll&#39;, &#39;British_Shorthair&#39;, 8), (&#39;keeshond&#39;, &#39;samoyed&#39;, 8), (&#39;Abyssinian&#39;, &#39;Sphynx&#39;, 7), (&#39;Maine_Coon&#39;, &#39;Persian&#39;, 7), (&#39;Siamese&#39;, &#39;Bombay&#39;, 6), (&#39;english_setter&#39;, &#39;basset_hound&#39;, 6), (&#39;leonberger&#39;, &#39;newfoundland&#39;, 6), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 6), (&#39;Siamese&#39;, &#39;Birman&#39;, 5), (&#39;american_bulldog&#39;, &#39;boxer&#39;, 5), (&#39;chihuahua&#39;, &#39;Sphynx&#39;, 5), (&#39;english_setter&#39;, &#39;english_cocker_spaniel&#39;, 5)] . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.634235 | 11.739998 | 0.890392 | 01:07 | . epoch train_loss valid_loss error_rate time . 0 | 3.723114 | 4.512056 | 0.935047 | 01:12 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 4.37e-03 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.318754 | 1.062278 | 0.310555 | 01:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.553926 | 1.065505 | 0.303789 | 01:11 | . 1 | 0.335815 | 0.722797 | 0.225304 | 01:11 | . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.100622 | 1.181886 | 0.358593 | 01:07 | . 1 | 0.533403 | 0.844639 | 0.246279 | 01:07 | . 2 | 0.319686 | 0.754717 | 0.233424 | 01:07 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=3.981071640737355e-05, lr_steep=6.309573450380412e-07) . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.256207 | 0.729607 | 0.222598 | 01:11 | . 1 | 0.234062 | 0.677414 | 0.212449 | 01:11 | . 2 | 0.220554 | 0.651485 | 0.198241 | 01:11 | . 3 | 0.211701 | 0.618529 | 0.187415 | 01:11 | . 4 | 0.180642 | 0.619414 | 0.190122 | 01:11 | . 5 | 0.172294 | 0.615591 | 0.184709 | 01:11 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.118661 | 1.226655 | 0.366712 | 01:07 | . 1 | 0.525033 | 0.785618 | 0.237483 | 01:06 | . 2 | 0.329720 | 0.706308 | 0.214479 | 01:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.261413 | 0.685852 | 0.207037 | 01:11 | . 1 | 0.258024 | 0.697651 | 0.215156 | 01:11 | . 2 | 0.227259 | 0.664356 | 0.203654 | 01:11 | . 3 | 0.223257 | 0.651984 | 0.197564 | 01:11 | . 4 | 0.189800 | 0.628065 | 0.193505 | 01:11 | . 5 | 0.171886 | 0.621270 | 0.184709 | 01:11 | . 6 | 0.164142 | 0.604195 | 0.182003 | 01:11 | . 7 | 0.149841 | 0.606904 | 0.186062 | 01:11 | . 8 | 0.135560 | 0.594956 | 0.177267 | 01:11 | . 9 | 0.124785 | 0.615664 | 0.180650 | 01:11 | . 10 | 0.124288 | 0.577947 | 0.172530 | 01:12 | . 11 | 0.123192 | 0.606744 | 0.182679 | 01:11 | . learn.recorder.plot_loss() . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.299714 | 1.316192 | 0.381597 | 01:06 | . 1 | 0.602832 | 1.305821 | 0.362652 | 01:06 | . 2 | 0.417395 | 1.141874 | 0.334912 | 01:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.243548 | 1.030849 | 0.306495 | 01:08 | . 1 | 0.331404 | 1.164932 | 0.326116 | 01:08 | . 2 | 0.254688 | 1.037982 | 0.288904 | 01:09 | . 3 | 0.150381 | 0.961565 | 0.264547 | 01:09 | . 4 | 0.078132 | 0.780013 | 0.227334 | 01:09 | . 5 | 0.053957 | 0.774100 | 0.221245 | 01:08 | .",
            "url": "https://domovou.github.io/fastpages_blog/2021/03/09/Uge_10.html",
            "relUrl": "/2021/03/09/Uge_10.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://domovou.github.io/fastpages_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Hotdog classifier",
            "content": ". !pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * . from fastbook import * from fastai.vision.widgets import * . configId = &quot;efb1b149-c219-4091-be6d-14a279c405c4&quot; . subscriptionKey = &quot;1be3ada0f2a649d087e9fb39798485a0&quot; . def search_images_bing_new(key, term, customConfigId, min_sz=128): url = &#39;https://api.bing.microsoft.com/v7.0/custom/images/search?&#39; + &#39;q=&#39; + term + &#39;&amp;&#39; + &#39;customconfig=&#39; + customConfigId + &#39;&amp;&#39; + &#39;count=150&#39; r = requests.get(url, headers={&#39;Ocp-Apim-Subscription-Key&#39;: key}) search_results = r.json() return L([img[&quot;thumbnailUrl&quot;] + &quot;.jpg&quot; for img in search_results[&quot;value&quot;][:150]]) . hotdogImages = search_images_bing_new(subscriptionKey, &quot;hotdog&quot;, configId) . firstHotdogImage = hotdogImages[0] hotdogDest = &quot;hotdog.jpg&quot; download_url(firstHotdogImage, hotdogDest) . hotdogImg = Image.open(hotdogDest) hotdogImg.to_thumb(128,128) . hotdogPath = Path(&#39;hotdogs&#39;) if not hotdogPath.exists(): hotdogPath.mkdir() download_images(&quot;hotdogs&quot;, urls = hotdogImages) . hotdogImageFiles = get_image_files(&quot;hotdogs&quot;) . failedHotdogs = verify_images(hotdogImageFiles) failedHotdogs . (#0) [] . foodImages = search_images_bing_new(subscriptionKey, &quot;food -hotdog&quot;, configId) foodPath = Path(&#39;food&#39;) if not foodPath.exists(): foodPath.exists() download_images(foodPath, urls=foodImages) foodImageFiles = get_image_files(foodPath) failedFood = verify_images(foodImageFiles) failedFood . (#0) [] . foods = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) foods = foods.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = foods.dataloaders(&quot;images&quot;) . dls.valid.show_batch(max_n=4, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(8) . epoch train_loss valid_loss error_rate time . 0 | 1.204669 | 0.573202 | 0.266667 | 00:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.551897 | 0.288518 | 0.100000 | 00:03 | . 1 | 0.439661 | 0.272065 | 0.050000 | 00:03 | . 2 | 0.362033 | 0.295560 | 0.050000 | 00:03 | . 3 | 0.320297 | 0.365100 | 0.066667 | 00:03 | . 4 | 0.263752 | 0.328156 | 0.083333 | 00:03 | . 5 | 0.222927 | 0.283832 | 0.050000 | 00:03 | . 6 | 0.191719 | 0.262306 | 0.050000 | 00:03 | . 7 | 0.169909 | 0.257237 | 0.050000 | 00:03 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . learn.export() . Image.open(&quot;hotdog.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;hotdog.jpg&quot;) pred,pred_idx,probs . (&#39;hotdogs&#39;, tensor(1), tensor([8.7614e-06, 9.9999e-01])) . Image.open(&quot;burger.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;burger.jpg&quot;) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: food; Probability: 0.9992&#39; . import ipywidgets as widgets from PIL import * . btn_upload = widgets.FileUpload() btn_upload . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . out_pl = widgets.Output() out_pl.clear_output() out_pl .",
            "url": "https://domovou.github.io/fastpages_blog/2020/02/18/Hotdog_finder.html",
            "relUrl": "/2020/02/18/Hotdog_finder.html",
            "date": " • Feb 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://domovou.github.io/fastpages_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://domovou.github.io/fastpages_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://domovou.github.io/fastpages_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}